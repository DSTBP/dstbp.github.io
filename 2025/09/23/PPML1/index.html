<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>[隐私保护] 隐私保护机器学习综述 - DSTBP Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="DSTBP Blog"><meta name="msapplication-TileImage" content="/images/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="DSTBP Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="引言学习生活中使用的 Chatgpt、手机解锁时的人脸识别、就医时医生参考的 AI 诊断建议…… 机器学习技术早已像水电一样渗透在生活的每个角落。从医疗影像分析到金融风控模型，从语音助手到自动驾驶系统，这些日益成熟的 AI 应用背后，都离不开海量数据的“喂养”。 机器学习中，模型效果往往和训练数据量成正比。为了让 AI 更聪明，商业公司不得不收集大量用户数据，你的语言特点、健康指标、喜爱偏好甚至社"><meta property="og:type" content="article"><meta property="og:title" content="[隐私保护] 隐私保护机器学习综述"><meta property="og:url" content="http://dstbp.com/2025/09/23/PPML1/"><meta property="og:site_name" content="DSTBP Blog"><meta property="og:description" content="引言学习生活中使用的 Chatgpt、手机解锁时的人脸识别、就医时医生参考的 AI 诊断建议…… 机器学习技术早已像水电一样渗透在生活的每个角落。从医疗影像分析到金融风控模型，从语音助手到自动驾驶系统，这些日益成熟的 AI 应用背后，都离不开海量数据的“喂养”。 机器学习中，模型效果往往和训练数据量成正比。为了让 AI 更聪明，商业公司不得不收集大量用户数据，你的语言特点、健康指标、喜爱偏好甚至社"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://dstbp.com/data/imgs/posts/PPML-Review/Thumbnail.png"><meta property="article:published_time" content="2025-09-23T05:55:20.000Z"><meta property="article:modified_time" content="2025-09-23T09:28:50.037Z"><meta property="article:author" content="DSTBP"><meta property="article:tag" content="数据安全"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://dstbp.com/data/imgs/posts/PPML-Review/Thumbnail.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://dstbp.com/2025/09/23/PPML1/"},"headline":"[隐私保护] 隐私保护机器学习综述","image":["https://dstbp.com/data/imgs/posts/PPML-Review/Thumbnail.png"],"datePublished":"2025-09-23T05:55:20.000Z","dateModified":"2025-09-23T09:28:50.037Z","author":{"@type":"Person","name":"DSTBP"},"publisher":{"@type":"Organization","name":"DSTBP Blog","logo":{"@type":"ImageObject","url":{"light":"/images/logo.jpg","dark":"/images/logo-dark.jpg"}}},"description":"引言学习生活中使用的 Chatgpt、手机解锁时的人脸识别、就医时医生参考的 AI 诊断建议…… 机器学习技术早已像水电一样渗透在生活的每个角落。从医疗影像分析到金融风控模型，从语音助手到自动驾驶系统，这些日益成熟的 AI 应用背后，都离不开海量数据的“喂养”。 机器学习中，模型效果往往和训练数据量成正比。为了让 AI 更聪明，商业公司不得不收集大量用户数据，你的语言特点、健康指标、喜爱偏好甚至社"}</script><link rel="canonical" href="http://dstbp.com/2025/09/23/PPML1/"><link rel="alternate" href="/atom.xml" title="DSTBP Blog" type="application/atom+xml"><link rel="icon" href="/images/favicon.svg"><link rel="stylesheet" href="/css/font/fontawesome/css/all.min.css"><link data-pjax rel="stylesheet" href="/js/imaegoo/highlight.js/11.7.0/styles/atom-one-light.css"><link data-pjax rel="stylesheet" href="/css/default.css"><!--!--><!--!--><script src="https://vercount.one/js" defer></script><link rel="stylesheet" href="/js/imaegoo/lightgallery/1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="/js/imaegoo/justifiedGallery/3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-K9WLQN9BFY" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-K9WLQN9BFY');</script><!--!--><!--!--><link rel="stylesheet" href="/js/imaegoo/cookieconsent/3.1.1/build/cookieconsent.min.css"><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><script type="text/javascript" src="/js/imaegoo/night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img class="logo-img" src="/images/logo.jpg" alt="DSTBP Blog" height="28"><img class="logo-img-dark" src="/images/logo-dark.jpg" alt="DSTBP Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives/">时间轴</a><a class="navbar-item" href="/categories/">分类</a><a class="navbar-item" href="/tags/">标签</a><a class="navbar-item" href="/messages/">留言板</a><a class="navbar-item" href="/friends/">友链</a><a class="navbar-item" href="/about/">关于</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i><span>  目录</span></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-09-23T05:55:20.000Z" title="2025/9/23 13:55:20">2025-09-23</time>发表</span><span class="level-item"><time dateTime="2025-09-23T09:28:50.037Z" title="2025/9/23 17:28:50">2025-09-23</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/">隐私保护</a><span> / </span><a class="link-muted" href="/categories/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/">数据安全</a></span><span class="level-item">1 小时读完 (大约12758个字)</span><span class="level-item leancloud_visitors" id="/2025/09/23/PPML1/" data-flag-title="[隐私保护] 隐私保护机器学习综述"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="twikoo_visitors"><i class="fa fa-spinner fa-spin"></i></span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">[隐私保护] 隐私保护机器学习综述</h1><div class="content"><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a><strong>引言</strong></h1><p>学习生活中使用的 Chatgpt、手机解锁时的人脸识别、就医时医生参考的 AI 诊断建议…… 机器学习技术早已像水电一样渗透在生活的每个角落。从医疗影像分析到金融风控模型，从语音助手到自动驾驶系统，这些日益成熟的 AI 应用背后，都离不开海量数据的“喂养”。</p>
<p>机器学习中，模型效果往往和训练数据量成正比。为了让 AI 更聪明，商业公司不得不收集大量用户数据，你的语言特点、健康指标、喜爱偏好甚至社交关系，都可能成为训练素材。但这里藏着一个核心矛盾：数据越丰富，隐私泄露风险就越高。</p>
<p>用户担心自己的敏感信息泄露，服务商怕模型核心参数被偷，而攻击者更直接，通过窃取敏感数据牟利，通过模型反推原始数据的技术也越来越成熟。尤其是医疗、金融这些敏感领域。这个矛盾尤为突出。</p>
<p>正是在这样的背景下，<strong>隐私保护机器学习（Privacy-Preserving Machine Learning, PPML）</strong> 这个新研究领域悄然崛起。它的核心目标听起来简单却意义重大：让机器在看不见原始数据的情况下完成学习，实现数据价值与隐私保护的双赢。</p>
<h1 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1. 基础知识"></a><strong>1. 基础知识</strong></h1><h2 id="1-1-机器学习"><a href="#1-1-机器学习" class="headerlink" title="1.1 机器学习"></a><strong>1.1 机器学习</strong></h2><h3 id="1-1-1-概述"><a href="#1-1-1-概述" class="headerlink" title="1.1.1 概述"></a><strong>1.1.1 概述</strong></h3><p>机器学习是一门交叉学科，涵盖统计学、计算机科学等多个领域。本质上，本质是对一类特定算法的总称。这类算法的核心目标的是：从海量历史数据中挖掘其中隐含的规律，并利用这些规律对新数据的结果进行预测。形式上，机器学习算法可以视为一个函数：输入是样本数据，输出是期望结果。</p>
<p>机器学习通常包括数据收集、预处理、模型训练与测试以及预测等环节，其中最核心的是<strong>训练（Training）和预测（Prediction）</strong>。训练阶段利用已有样本不断迭代优化模型参数，使模型逐步学习并捕捉数据中的规律；预测阶段则将训练好的模型应用于新数据，输出相应结果。机器学习的关键在于不仅要在训练数据上表现优异，还要能有效推广到未见过的数据，这种能力被称为<strong>泛化能力</strong>。</p>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/machine learning process.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图1.1 机器学习解决问题过程</span></center>

<h3 id="1-1-2-分类"><a href="#1-1-2-分类" class="headerlink" title="1.1.2 分类"></a><strong>1.1.2 分类</strong></h3><p>根据学习方式的不同（训练数据是否有标签），机器学习可分为 4 类：</p>
<ul>
<li><p><strong>监督学习（Supervised Learning）</strong>：有老师带，照着答案学</p>
<p>就像学生做题时有标准答案，模型训练前，所有数据都带正确答案（标签），比如图片识别时，训练集每张图都标注正确的答案，这张是猫、这张是狗；垃圾短信识别，每条短信标注是否是垃圾短信等等。模型通过对比数据和标签学习规律，学会后举一反三给新数据贴标签或预测结果。</p>
<p>常用于<strong>分类</strong>（例如垃圾邮件分类、图片识别分类）和<strong>回归</strong>（用一组特征来预测一个连续的结果，例如用房间面积、位置等特征来预测房价）。</p>
<p>代表性算法包括支持向量机（SVM）、决策树、随机森林、K 近邻、朴素贝叶斯、线性回归、神经网络（CNN、RNN）等。</p>
</li>
<li><p><strong>半监督学习（Semi-Supervised Learning）</strong>：少数老师带，多数靠自学</p>
<p>数据里只有一小部分带标签（比如 1000 条数据里只有 100 条有答案），剩下大部分没标签。模型先靠少量带标签数据学基础规律，再用这个规律去推测无标签数据的可能标签，最后结合两者进一步优化。</p>
<p>常用于<strong>标签获取成本高</strong>的场景，比如医疗数据（专家标标签耗时）、稀有事件分类（地震数据）。</p>
<p>代表性算法包括半监督支持向量机（S3VM）、图半监督学习（Label Propagation, Graph-based SSL）等。</p>
</li>
<li><p><strong>无监督学习（Unsupervised Learning）</strong>：没老师教，自己找规律</p>
<p>训练数据完全没标签，模型只能靠自己分析数据的内在特征，比如相似性、分布规律，把数据分组、找异常，或简化数据维度。</p>
<p>常见于<strong>聚类</strong>（把相似数据分组，例如电商用户分群）、<strong>降维</strong>（简化数据，保留关键信息，例如刻画用户关键特征画像）、<strong>关联规则</strong>（找数据间的关联，例如个性化推荐）。</p>
<p>典型方法有主成分分析（PCA）、k-means 聚类、Apriori 算法等。</p>
</li>
<li><p><strong>强化学习（Reinforcement Learning）</strong>：靠试错学，有奖有罚</p>
<p>没有直接的正确答案，模型通过“试错”与环境交互，基于奖惩机制改进行为策略，以找到最优决策方案。比如训练机器人走迷宫，避开障碍，就给奖励加分；撞到墙壁、走进死胡同，就给惩罚减分，模型的目标是通过不断试错，找到得最多分的策略。</p>
<p>常用于<strong>控制</strong>（做决策和路径规划，例如自动驾驶，汽车在模拟环境里行驶，避开行人、遵守红绿灯就给正反馈，压线、碰撞就给负反馈，慢慢学会安全驾驶路线）与<strong>博弈问题</strong>（AlphaGo 下围棋、游戏人机 AI）。</p>
<p>代表算法：Q - 学习（Q-Learning）、SARSA 算法、深度强化学习（DQN）、近端策略优化（PPO）。</p>
</li>
</ul>
<p>根据训练集数据存放方式和训练架构（数据是否迁移），机器学习可分为 3 类：</p>
<ul>
<li><p><strong>集中式学习（Centralized Learning）</strong>：所有数据堆一起，在一个中心训练</p>
<p>所有训练数据集中到一个地方（服务器），模型直接在这个中心服务器上用所有数据训练。比如实验室的小数据集研究，用 1 万张图片训练图像识别模型，所有图片都存在实验室的电脑里，直接在这台电脑上训练。</p>
<p>常用于数据量不大、数据隐私要求不高的场景。</p>
<p>所有标准机器学习算法都可以用集中式学习。</p>
</li>
<li><p><strong>分布式学习（Distributed Learning）</strong>：数据分存在多个节点，训练时汇总协作</p>
<p>数据量太大或模型太复杂，单机算不动，就将数据分布到多个子节点，每个子节点先用自己的数据训练局部模型，再把局部模型的参数（相当于模型学到的规律）传到中心，中心把所有子节点的参数汇总优化，得到“全局模型”，再把全局模型发回子节点继续训练，循环到模型达标。</p>
<p>常用于数据量大、需要多节点协作，但数据隐私要求中等的场景，比如大模型训练（GPT 类模型）、大型互联网公司的用户行为分析、物流网络路径优化。</p>
<p>代表算法有分布式梯度下降（DGD）、参数服务器（Parameter Server）架构下的各类算法（比如分布式随机森林、分布式 CNN）。</p>
</li>
<li><p><strong>联邦学习（Federated Learning）</strong>：数据留在本地，只传参数不传数据</p>
<p>数据严格留在原始节点（比如用户个人手机中的数据），不上传数据，每个节点用本地数据训练得到局部模型后，只上传模型参数，保护隐私。</p>
<p>常用于数据隐私要求极高、数据分散在多个独立主体的场景，比如医疗数据、用户个性化数据、金融企业数据。</p>
<p>代表算法：联邦平均算法（FedAvg）、联邦随机森林、联邦梯度提升树（FedXGBoost）。</p>
</li>
</ul>
<p><strong>总结</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">分类维度</th>
<th style="text-align:left">具体分类名称</th>
<th style="text-align:left">核心特点</th>
<th style="text-align:left">典型场景</th>
<th style="text-align:left">代表算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">按训练数据是否有标签</td>
<td style="text-align:left">监督学习</td>
<td style="text-align:left">有标签，对照数据与标签学习</td>
<td style="text-align:left">分类与回归</td>
<td style="text-align:left">SVM、随机森林、线性回归、CNN</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">半监督学习</td>
<td style="text-align:left">少量标签 + 大量无标签，结合两者优化</td>
<td style="text-align:left">标签获取成本高</td>
<td style="text-align:left">半监督 SVM（S3VM）、标签传播算法</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">无监督学习</td>
<td style="text-align:left">无标签，自主挖掘数据规律</td>
<td style="text-align:left">聚类与降维</td>
<td style="text-align:left">k-means、PCA、Apriori 算法</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">强化学习</td>
<td style="text-align:left">无固定答案，靠与环境交互试错</td>
<td style="text-align:left">控制与博弈</td>
<td style="text-align:left">Q-Learning、DQN、PPO</td>
</tr>
<tr>
<td style="text-align:left">按训练数据存放方式 &amp; 训练架构</td>
<td style="text-align:left">集中式学习</td>
<td style="text-align:left">数据集中存，单中心训练</td>
<td style="text-align:left">数据量不大、数据隐私要求不高</td>
<td style="text-align:left">所有标准算法（如逻辑回归、k-means）</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">分布式学习</td>
<td style="text-align:left">数据分存多节点，传参数汇总训练全局模型</td>
<td style="text-align:left">数据量大、隐私要求中等</td>
<td style="text-align:left">分布式梯度下降（DGD）、参数服务器</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">联邦学习</td>
<td style="text-align:left">数据留本地，只传参数，保护隐私</td>
<td style="text-align:left">数据分散、隐私要求高</td>
<td style="text-align:left">联邦平均（FedAvg）、联邦随机森林</td>
</tr>
</tbody>
</table>
</div>
<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">表1.1 机器学习分类表</span></center>

<h1 id="2-机器学习中的隐私威胁"><a href="#2-机器学习中的隐私威胁" class="headerlink" title="2. 机器学习中的隐私威胁"></a><strong>2. 机器学习中的隐私威胁</strong></h1><h2 id="2-1-隐私数据"><a href="#2-1-隐私数据" class="headerlink" title="2.1 隐私数据"></a><strong>2.1 隐私数据</strong></h2><p>在机器学习中，隐私问题不仅仅局限于原始数据本身，还包括整个机器学习流程中可能泄露用户或组织敏感信息的环节。</p>
<ol>
<li><p><strong>训练数据隐私（Data Privacy）</strong></p>
<p>指模型训练所依赖的数据中包含的个人身份信息、敏感内容等，常见包括：</p>
<ul>
<li>标识符（Identifiers）：直接识别个人的唯一信息，比如姓名、身份证号、手机号、邮箱。</li>
<li>准标识符（Quasi-identifiers）：单独看不唯一，但组合起来可能识别个人，比如出生日期、性别、邮编。</li>
<li>敏感信息（Sensitive Attributes）：用户不愿公开或涉及隐私的内容，比如健康状况、收入、消费习惯、宗教信仰。</li>
</ul>
</li>
<li><p><strong>模型隐私（Model Privacy）</strong></p>
<p>模型本身的结构、参数和训练过程也可能包含敏感信息。比如机器学习中模型训练算法、模型拓扑结构、模型权重参数、激活函数以及超参数。攻击者一旦拿到这些信息，就可以通过某些手段推断训练数据是否包含某个用户。比如一个人脸识别模型，如果被攻击，就可能泄露出训练时的原始人脸数据。</p>
</li>
<li><p><strong>预测结果隐私（Inference Privacy）</strong></p>
<p>当用户使用已训练好的模型预测数据时，输入的数据，模型反馈的结果也可能包含用户不愿公开的敏感信息，比如 AI 诊病，患者输入的数据和诊断结果这些都是敏感信息。</p>
</li>
</ol>
<h2 id="2-2-隐私威胁"><a href="#2-2-隐私威胁" class="headerlink" title="2.2 隐私威胁"></a><strong>2.2 隐私威胁</strong></h2><h3 id="2-2-1-概述"><a href="#2-2-1-概述" class="headerlink" title="2.2.1 概述"></a><strong>2.2.1 概述</strong></h3><p>分析机器学习中的隐私威胁，就像侦破一起技术案件，首先需要明确三个关键问题：攻击者的目标是什么、他们能采用哪些手段、以及敌手已经掌握了哪些信息作为突破口？</p>
<p>首先根据攻击目标的不同，机器学习中的隐私威胁分为针对<strong>数据的威胁</strong>和针对<strong>模型的威胁</strong>。针对数据的威胁核心目标是获取训练数据中的敏感信息。而对模型的威胁是指试图窃取模型的结构、参数或训练逻辑，从而复制出功能相似的模型。</p>
<p>攻击者对模型的了解程度，直接决定了他们能采用的攻击手段。知道锁的内部结构和只知道是一个锁，破解难度和方法截然不同。如果敌手掌握模型的“全套说明书”，包括模型结构、全部参数，甚至部分训练数据的分布规律。那么攻击者可以直接分析模型的漏洞，设计针对性攻击。这种称之为<strong>白盒攻击（White-box attack）</strong>，主要针对数据的威胁。而反之黑盒攻击者对模型内部一无所知，只能通过“输入提问 - 观察输出”的方式摸索规律。<strong>黑盒攻击（Black-box attack）</strong>更像强化学习，只不过没有奖惩，只能通过大量试错积累信息。常见于针对模型的威胁。</p>
<p>除了目标和知识，敌手的实际能力也决定了威胁的严重程度。我们将敌手分为<strong>强敌手</strong>和<strong>弱敌手</strong>，强敌手拥有较强的干预能力，甚至能参与到模型的训练过程中。比如联邦学习中敌手可能作为参与方提交恶意本地模型，或者在训练数据植入后门。弱敌手没有能力干预训练，只能在模型部署后通过查询接口进行试探，他们可以通过发送精心设计的输入，观察模型的输出规律，进而推断敏感信息。比如 GPT 大模型中的提示词注入，就是典型的弱敌手手段。</p>
<p>无论是设计防御策略还是评估风险，都必须先明确敌手模型，只有精准定位威胁来源，才能针对性地构建防御体系。</p>
<p>本文以攻击目标为例，对隐私威胁进行具体分类与讨论：</p>
<ul>
<li><strong>数据威胁</strong>：数据窃取攻击、模型反演攻击、属性推理攻击，成员推理攻击。其中，属性推理攻击、数据窃取攻击既可能发生在训练阶段，也可能出现在预测阶段；模型反演攻击和成员推理攻击主要发生在预测阶段。</li>
<li><strong>模型威胁</strong>：主要是模型提取攻击，通常发生在预测阶段，攻击者通过不断查询模型来逼近其内部参数或功能。</li>
</ul>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/The stage with privacy threats.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图2.1 机器学习隐私威胁发生阶段</span></center>

<h3 id="2-2-2-数据窃取攻击"><a href="#2-2-2-数据窃取攻击" class="headerlink" title="2.2.2 数据窃取攻击"></a><strong>2.2.2 数据窃取攻击</strong></h3><p>这类攻击泛指训练数据泄露，主要发生在数据收集与数据预处理两个阶段。在数据收集阶段，机器学习服务商往往需要整合来自不同平台的大量信息，例如用户在购物应用中购买物品、在短视频平台浏览历史、在社交应用中发布动态。单看这些数据似乎分散在不同平台，但都指向同一个用户。多源数据的融合虽能帮助平台更好地刻画用户，却同时放大了隐私泄露风险，攻击者正是利用这一点，着重挖掘数据收集阶段的关联信息，进而拼接出完整的个人情况。</p>
<p>同样在数据预处理阶段，不同数据集的所有者可能需要联合进行清洗、集成与转换等操作，这通常依赖索引或查询。若对数据进行加密，会影响这些操作的正常执行，因此云端应用常使用未加密的静态数据，从而留下隐患。一旦存在不诚实的参与方，就可能借机窃取其他参与方的数据，造成严重隐私泄漏。</p>
<h3 id="2-2-2-模型反演攻击"><a href="#2-2-2-模型反演攻击" class="headerlink" title="2.2.2 模型反演攻击"></a><strong>2.2.2 模型反演攻击</strong></h3><p>模型反演攻击 / 模型逆向攻击（Model Inversion Attack，MIA）是指攻击者利用模型的输出（如预测概率、置信分布、中间层特征甚至梯度信息），反向恢复或重构训练数据中的隐私信息。</p>
<p>具体攻击手段随敌手的背景知识而异，如果是白盒攻击，那么敌手可以直接访问模型结构或梯度，通过对输入进行优化（反向传播或梯度上升）来最大化目标类别的输出概率，从而逐步逼近并重构出训练样本。如果是黑盒，就需要只能多次查询模型 API，利用生成对抗网络（GAN）或优化算法生成与训练数据相似的伪造样本。</p>
<p>最容易理解的 MIA 场景，就是人脸识别模型。假设攻击者能访问一个返回“是张三概率”的模型。那么敌手可以先随便找一张模糊的人脸图像作为初始输入，传给模型，得到是张三的概率，可能只有 1%；然后轻微调整这张模糊图的像素（如调整某处亮度或轮廓），再传给模型，观察概率变化。如果概率上升，就沿该方向继续调整，若下降则改变策略；反复上面的操作，让模型不断判断，根据输出的概率修正输入，最后重构出训练时用的张三清晰人脸照。</p>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Model Inversion Attack.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图2.2 根据输入噪声</span></center>，恢复训练集特定类别的代表性人脸

### **2.2.3 成员推理攻击**
成员推理攻击（Membership Inference Attack，MIA）指对于给定的数据，攻击者通过模型预测结果，判断该数据是否属于训练集的原数据。

其核心依据是：模型在训练数据和未见数据上的预测表现往往存在差异。对训练数据，模型的预测概率可能更高、误差更小；而对新数据，预测结果不确定性更多。

MIA 常用的一种方法是训练多个与目标模型结构相似的**影子模型（shadow models）**以模拟目标模型的行为：攻击者先用已知数据训练若干影子模型，分别记录影子模型对“见过（训练过）”样本与“未见过”样本的输出分布与规律，从而构建带标签的训练集（标签表示样本是否在训练集中）；再用这些带标签的数据训练一个二分类的攻击判别器（attack classifier），用于区分“模型见过”与“模型没见过”。实际攻击时，攻击者将待测样本送入目标模型，取其输出结果作为特征，交由攻击判别器判断并给出结论：该样本很可能是目标模型的训练数据，或很可能不是。

这种攻击的常见场景是医疗 AI 模型中，比如某癌症预测模型，攻击者拿一个患者的检查报告问模型是不是癌症，如果通过攻击判断出这个患者的报告是训练数据的一部分，那么就会泄露患者曾有癌症相关检查的隐私病史。

<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Membership Inference Attack.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图2.3 成员推理攻击架构图</span></center>

<h3 id="2-2-4-属性推理攻击"><a href="#2-2-4-属性推理攻击" class="headerlink" title="2.2.4 属性推理攻击"></a><strong>2.2.4 属性推理攻击</strong></h3><p>属性推理攻击（Property Inference Attack, PIA）指攻击者通过访问目标模型，推断训练数据中未直接公开的统计性属性（如性别比例、疾病分布等）。这类属性并不直接出现在模型输入或标签上，而是敏感信息分布特征。（attacker cares about distributional / auxiliary features）。</p>
<p>根据攻击者的访问权限，PIA 分为两类：</p>
<ul>
<li><strong>黑盒访问</strong>：攻击者只能通过 API 查询，获取输出标签或概率向量；</li>
<li><strong>白盒访问</strong>：攻击者能直接访问模型的参数、权重或训练梯度（典型于联邦学习中的恶意参与方）。</li>
</ul>
<p>其原理与成员推理攻击类似：攻击者自建多个训练集（部分包含目标属性，部分不包含），在相同或相似的模型结构上训练影子模型，记录其在同一输入上的行为特征（输出概率、置信度分布、损失等），再以此训练一个<strong>属性分类器</strong>，用于区分“含属性/不含属性”。最终，攻击者便能通过该分类器推断目标模型训练数据是否包含某一敏感属性。</p>
<p>常用于医疗模型和统计分析中，比如在医疗模型中，推断某医院的癌症患者占比，或某药物临床试验中副作用发生率。或者是攻击用户兴趣推荐模型，获取平台用户的性别分布隐私。</p>
<h3 id="2-2-5-模型提取攻击"><a href="#2-2-5-模型提取攻击" class="headerlink" title="2.2.5 模型提取攻击"></a><strong>2.2.5 模型提取攻击</strong></h3><p>在机器学习云服务场景中，云服务商拥有预先训练好的处理模型，用户使用模型，服务商会对用户的访问收取一定费用。此时模型内部信息本身构成敏感资产，用户对模型的访问通常被视为黑盒访问。模型提取攻击（Model Extraction Attack，MEA）即通过一些手段窃取机器学习模型结构或模型内部参数。具体目的可以分为三类：</p>
<ol>
<li>免费使用模型：不想付费，只想白嫖。</li>
<li>窃取训练数据：拿到隐私训练数据牟利。</li>
<li>逃逸攻击：常见于机器学习进行恶意检测场景，攻击者获取目标模型之后，可以根据模型信息构造对抗样本来逃避安全检测。</li>
</ol>
<p>常见窃取方法是通过大量查询收集输入—输出对，然后用这些数据训练一个近似的替代模型（surrogate），借此推断原模型的行为与内部信息并进一步发起攻击或滥用。</p>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Model Extraction Attack.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图2.4 模型提取攻击架构图</span></center>

<h1 id="3-机器学习中的隐私保护"><a href="#3-机器学习中的隐私保护" class="headerlink" title="3. 机器学习中的隐私保护"></a><strong>3. 机器学习中的隐私保护</strong></h1><h2 id="3-1-隐私保护思想"><a href="#3-1-隐私保护思想" class="headerlink" title="3.1 隐私保护思想"></a><strong>3.1 隐私保护思想</strong></h2><p>正所谓有矛必有盾，针对前述隐私威胁，研究人员也针对性地想出了不少防御办法。核心思路无外乎模糊数据细节、隐藏敏感信息、干扰攻击逻辑、强化数据安全这几类，研究人员将其总结为泛化、匿名化、随机扰动和加密。</p>
<ol>
<li><p><strong>泛化（Generalization）</strong></p>
<p>通过模糊化处理隐藏细节，把数据从“精确”变为“模糊”，在保留语义一致性的同时弱化攻击者识别个体的能力。</p>
<p>这种思想在统计回归中应用广泛，例如人口普查分析模型中，原始数据“张三，25 岁，住在北京市朝阳区某小区”可泛化为“张三，20–30 岁，住在北京市朝阳区”。这样仍能支持统计分析，但无法暴露个人的精确信息。</p>
<p>缺点是隐私保护增强的同时，数据的价值也会有所降低。</p>
</li>
<li><p><strong>匿名（Anonymization）</strong></p>
<p>通过去除或掩盖可识别属性来保护隐私。典型方法是 <strong>k-匿名</strong>，其要求在发布的数据中，隐藏唯一标识符（如姓名、身份证号），并确保准标识符（比如年龄 + 性别）中存在至少 k 条不可区分的记录。这样就算攻击者知道某人的准标识符，也分不清数据里哪条是 TA 的。</p>
<p>常用于表格型数据建模，例如用户数据收集分析，名字只展示姓、手机号仅展示前三位和后四位；或在共享医疗数据时隐藏患者身份。</p>
<p>缺点跟泛化一样，数据可用性下降。</p>
</li>
<li><p><strong>随机扰动（Perturbation）</strong></p>
<p>在原始数据中引入噪声，使得新数据与原始数据产生差异，但整体统计规律（如均值、比例）不变，从而让攻击者“看不清”，掩盖个体真实情况。噪声越大，隐私保护越强，但数据实用性越差。需要考虑在隐私与实用之间权衡。</p>
<p>应用场景包括图像识别和大模型训练，比如在照片上加轻微马赛克或滤镜，整体还能看清楚（比如知道是个男生或女生），但看不出具体的脸；在模型预测结果里加噪声，防止攻击者反推出训练样本。</p>
<p>噪声越大，隐私保护越强，但数据可用性也越低，需要在隐私与实用之间权衡。</p>
</li>
<li><p><strong>加密（Encryption）</strong></p>
<p>最简单粗暴也是最常用的方法，通过密钥将数据转化为不可读的密文，只有拥有密钥者才能恢复原始内容。分为对称和非对称加密。</p>
<p>典型场景是云端机器学习，用户数据上传云端前加密，以防止服务商窥视原始内容。</p>
<p>该方法加密保护最强，但由于密文不可读，导致更难分析数据。需要结合同态加密、多方安全计算（SMC）等技术，才能在加密状态下安全计算。</p>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">防御方法</th>
<th style="text-align:left">核心思路</th>
<th style="text-align:left">典型应用场景</th>
<th style="text-align:left">主要优缺点</th>
<th style="text-align:left">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">泛化</td>
<td style="text-align:left">精确数据转模糊，保语义一致性、弱个体识别</td>
<td style="text-align:left">统计数据（数据区间化）</td>
<td style="text-align:left">优点：支持统计分析；   缺点：隐私增强伴随数据价值降低</td>
<td style="text-align:left">二元搜索</td>
</tr>
<tr>
<td style="text-align:left">匿名化</td>
<td style="text-align:left">去 / 掩盖可识别属性</td>
<td style="text-align:left">用户数据、医疗数据</td>
<td style="text-align:left">优点：防止个体定位；   缺点：数据可用性下降</td>
<td style="text-align:left">k-匿名</td>
</tr>
<tr>
<td style="text-align:left">随机扰动</td>
<td style="text-align:left">加噪声，保整体统计规律</td>
<td style="text-align:left">图像处理、模型训练</td>
<td style="text-align:left">优点：护个体隐私不破坏整体规律；   缺点：噪声与实用性需权衡</td>
<td style="text-align:left">差分隐私</td>
</tr>
<tr>
<td style="text-align:left">加密</td>
<td style="text-align:left">加密成密文</td>
<td style="text-align:left">云端机器学习</td>
<td style="text-align:left">优点：保护强度高；   缺点：密文难分析</td>
<td style="text-align:left">同态加密</td>
</tr>
</tbody>
</table>
</div>
<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">表3.1 隐私保护主要思想归纳</span></center>

<h2 id="3-2-隐私保护技术"><a href="#3-2-隐私保护技术" class="headerlink" title="3.2 隐私保护技术"></a><strong>3.2 隐私保护技术</strong></h2><p>根据上面的思想，针对不同类型的隐私威胁，研究者设计了差异化的防御策略：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">威胁形式</th>
<th style="text-align:left">防范措施</th>
<th style="text-align:left">防范对象</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">数据泄漏</td>
<td style="text-align:left">同态加密、安全多方计算协议</td>
<td style="text-align:left">服务商、恶意参与方</td>
</tr>
<tr>
<td style="text-align:left">模型反演攻击</td>
<td style="text-align:left">同态加密</td>
<td style="text-align:left">恶意用户</td>
</tr>
<tr>
<td style="text-align:left">成员推理攻击</td>
<td style="text-align:left">差分隐私</td>
<td style="text-align:left">恶意用户、恶意参与方</td>
</tr>
<tr>
<td style="text-align:left">模型提取攻击</td>
<td style="text-align:left">假神经元、对模型参数添加随机扰动</td>
<td style="text-align:left">恶意用户</td>
</tr>
</tbody>
</table>
</div>
<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">表3.2 针对各种隐私威胁的防范措施</span></center>

<p>在实际应用中，数据训练阶段采取泛化和匿名两种思想进行防护，主要技术是<strong>数据脱敏</strong>。而预测阶段主要采用加密和扰动两种思想，其中，加密主要依赖密码学技术，常见的有安全多方计算、同态加密等；扰动则主要通过差分隐私机制实现。技术主要可分为三类：</p>
<ul>
<li><strong>基于差分隐私</strong>：属于数据失真方法，通过生成合成数据或注入噪声来保护隐私；</li>
<li><strong>基于同态加密</strong>：允许在密文状态下直接进行计算，保证数据在加密存储和传输中的安全；</li>
<li><strong>基于安全多方计算</strong>：在多方协同计算中避免任何单一方获取完整数据，从而保护参与方隐私。</li>
</ul>
<p>在复杂场景中，往往还需要将这些方法结合使用，例如将安全多方计算与差分隐私联合，或将同态加密与安全多方计算融合，以达到更好的隐私保护效果。</p>
<h3 id="3-2-1-数据脱敏"><a href="#3-2-1-数据脱敏" class="headerlink" title="3.2.1 数据脱敏"></a><strong>3.2.1 数据脱敏</strong></h3><p>数据脱敏（Data Anonymization）是一类通过处理数据来降低隐私泄露风险的技术，目标是在保留统计特征和可用性的同时，最大限度地隐藏个体信息。理想状态下，脱敏后的数据无法再与特定个人建立联系，即便攻击者结合外部信息也无法完成身份重识别。然而，这也可能导致数据持有者自身难以再识别数据主体，因此在匿名化方法中通常需要权衡与改进，以提升数据可用性。</p>
<p>常见的数据脱敏方法包括：</p>
<ul>
<li><strong>直接去标识化</strong>：移除数据中可直接识别身份的属性，如姓名、地址、身份证号等。典型技术有 k-匿名、l-多样性和 t-接近性。</li>
<li><strong>数据泛化</strong>：将精确值转化为更粗粒度或范围值，例如把精确日期泛化为月份或年份，从而降低再识别的可能。</li>
<li><strong>数据交换</strong>：在数据集中交换部分属性的取值，以打乱原有的直接关联关系，同时保留对模型训练的整体可用性。</li>
</ul>
<p>虽然上述方法能在隐私保护和数据可用性之间取得一定平衡，但只要数据仍保留分析价值，就意味着一旦发生泄露，攻击者可能通过推断或恢复技术重新识别个体，隐私风险依然存在。</p>
<h3 id="3-2-2-差分隐私"><a href="#3-2-2-差分隐私" class="headerlink" title="3.2.2 差分隐私"></a><strong>3.2.2 差分隐私</strong></h3><p>差分隐私是 2006 年 Dwork<sup>[11]</sup> 等人提出的一种隐私保护理论框架。它的核心思想是主要是<strong>随机扰动</strong>：让算法在对整个数据集进行分析时，结果几乎不依赖于任何一个具体人的数据是否存在。即“<strong>整体有用、个体隐身</strong>”。</p>
<p>要实现个人隐身，关键方法是在计算结果里加入精心设计的随机噪声。以小区统计“居民月收入均值”为例，若你家真实收入为 4 万元，系统在收集时人为加入一个 -500 至 +500 元的噪声，得到 39600 元。其他住户数据也会做类似处理。由于噪声在整体上正负抵消，最终均值仍接近真实值，不影响统计用途，但单个用户的真实收入却被有效隐藏。</p>
<p>前文提到，增加随机噪声需要控制保护强度和数据可用性的平衡，用数学公式进行量化，直观表达如下：</p>
<script type="math/tex; mode=display">
pr\left[A(T)\in S\right]\leqslant e^{\varepsilon}pr\left[A\left(T^{\prime}\right)\in S\right]+\delta</script><ul>
<li>$ T /\ T^{\prime} $：两个“相邻”的数据集（只相差几个样本）。比如 $ T $ 含有小明的记录，而 $ T^{\prime} $ 没有小明，其他都一样。</li>
<li>$ A $：某种算法，可能是一个函数（比如求平均值），也可能是一个机器学习训练过程。</li>
<li>$ S $：算法可能输出的结果集合（比如某个范围内的平均值）。</li>
<li>$ \Pr[A(T)\in S] $：在数据集 $ T $ 上运行算法 $ A $，得到结果落在集合 $ S $ 内的概率。</li>
<li>$ \varepsilon $：<strong>隐私预算</strong>，控制隐私保护强度。<ul>
<li>$ \varepsilon $ 越小，隐私保护越强，但数据可用性越低；</li>
<li>$ \varepsilon $ 越大，隐私保护越弱，但数据越准。</li>
</ul>
</li>
<li>$ \delta $：<strong>容忍度</strong>，允许不满足隐私的概率，相当于“最多 δ 的几率不满足隐私保护”。比如 $ \delta = 0.001 $，代表 1000 次查询中，最多有 1 次 $ T $ 和 $ T^{\prime} $ 的结果差异稍大。</li>
</ul>
<p>如果 $ \delta=0 $，就是严格的 $ \varepsilon $-差分隐私，如果 $ \delta&gt;0 $，就是$  (\varepsilon, \delta) $-宽松差分隐私。后者更常用。</p>
<p>不过，差分隐私也存在不足：</p>
<ul>
<li>普通机器学习模型（比如线性回归，属于凸模型）：可简单理解成“碗”：小球无论从哪放入，最终都会滚到唯一的碗底（最优解）。梯度能指示坡度和下坡方向，梯度下降就是小球滚向碗底的过程。机器学习的目标，是借训练集规划路线，通过梯度下降逐步调整参数、减小误差，最终找到贴合数据的直线，让模型在训练数据上的预测误差最小。但数据若加太多噪声，像接近 “患病 / 不患病” <strong>临界值</strong>这类决策边界附近的样本可能标错，会让训练集的路线变模糊，导致模型找不到正确直线（不收敛）。</li>
<li>深度学习模型（比如 CNN，属于非凸模型）：结构更复杂、需要更多次的访问数据（比如迭代 1000 次不断调整参数），每次迭代都要加噪声，<strong>噪声会累积</strong>，更难训练出好用的模型。</li>
</ul>
<p>为缓解这些问题，研究者提出了多种改进方法：</p>
<ol>
<li><p><strong>分场景针对性加噪声</strong></p>
<ul>
<li>输出扰动：在模型预测结果加噪声（如把“患病概率 80%”改为 82%）；</li>
<li>目标扰动：在训练目标中加噪声（如把“预测误差最小”改为“误差 &lt; 0.5”）；</li>
<li>梯度扰动（DP-SGD）：深度学习训练时，给梯度加噪声（比如原本要把参数调 + 0.1，改成调 + 0.12）。</li>
</ul>
</li>
<li><p><strong>隐私会计</strong>（Privacy Accounting）</p>
<p> 为模型设定隐私预算阈值，引入一个会计， 每次访问数据都会记账，即记录消耗的隐私预算 $ \varepsilon $，保证总消耗不超出上限。</p>
</li>
<li><p><strong>调整隐私平衡</strong></p>
<p>最常见的就是$  (\varepsilon, \delta) $-宽松差分隐私，除此之外，还有 KL 散度差分隐私、雷尼差分隐私等，本质都是在隐私和可用性之间找更精细的平衡。</p>
</li>
</ol>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Differential Privacy.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图3.1 训练目标扰动直观体现</span></center>

<h3 id="3-2-1-同态加密"><a href="#3-2-1-同态加密" class="headerlink" title="3.2.1 同态加密"></a><strong>3.2.1 同态加密</strong></h3><p>传统加密（如 AES、RSA）只能把数据“锁起来”。一旦需要使用数据（如训练模型），就必须先解密，此时数据可能被窃取。而<strong>同态加密（Homomorphic Encryption，HE）</strong>的独特之处在于：即使数据处于加密状态，也能直接进行运算，最终解密结果与在原始数据上运算的结果一致。这一概念最早由 Rivest<sup>[12]</sup> 等人在 1978 年提出。其基本形式为：</p>
<script type="math/tex; mode=display">
Enc(f(m_1,m_2))=f(Enc(m_1),Enc(m_2))</script><center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Homomorphic Encryption.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图3.2 同态加密示意图</span></center>

<p>根据支持的运算类型，同态加密分为两类：</p>
<ol>
<li><p><strong>部分同态加密（PHE）</strong></p>
<p>仅支持<strong>加法或乘法中的一种</strong>（如 Paillier 支持加法，RSA 支持乘法），可用于简单的统计任务（如求和、乘积）。</p>
</li>
<li><p><strong>全同态加密（FHE）</strong></p>
<p>同时支持加法和乘法，理论上可以表达任意计算。2009 年 Gentry<sup>[13]</sup> 基于“理想格”首次实现全同态加密。尽管技术已迭代到第三代，但仍存在<strong>效率极低</strong>的问题：一次简单的 10 次乘法可能耗时是普通计算的 1000 倍，且加密后的数据量可能膨胀 10-100 倍。</p>
</li>
</ol>
<p>因此，现实中 90% 以上的应用依然采用 PHE，FHE 仍主要处于实验室优化阶段。且同态加密在机器学习中主要用在<strong>预测阶段</strong>，比如用训练好的模型给用户做诊断、评分），而非训练阶段，因为训练需要大量复杂运算，同态加密目前扛不住。</p>
<p>举个例子：</p>
<ul>
<li>医院已有一个肺癌预测模型（部署在服务器上）；</li>
<li>你做了 CT 检查，但不希望服务器直接获取原始影像数据；</li>
<li>你用同态加密将 CT 数据加密为密文，密文上传到服务器；</li>
<li>服务器直接将密文传给模型，得到加密预测结果；</li>
<li>你接收并解密，得到“肺癌风险 10%”。在此过程中，服务器从未接触到你的真实数据，隐私得到保护。</li>
</ul>
<p>然而，HE 在应用中面临三大瓶颈：</p>
<ul>
<li><strong>运算支持有限</strong>：仅支持加法和乘法，但机器学习需要复杂操作（比较、ReLU 激活、最大值、指数运算等）。</li>
<li><strong>噪声问题</strong>：同态加密在运算时会产生微小噪声（计算误差），特别是乘法，噪声会不断放大，机器学习模型（尤其是深度学习）要做几十、上百次乘法运算，很容易触发“噪声超阈值”的问题。</li>
<li><strong>效率低：</strong><ul>
<li>加密后的数据会急剧膨胀（几 MB 甚至 GB）。</li>
<li>运算速度比原始数据慢 100-1000 倍，训练神经网络几乎不可行。</li>
</ul>
</li>
</ul>
<p>研究者们针对上述难题，总结了两种核心解决思路，目前已经在部分场景落地：</p>
<ul>
<li><strong>任务拆分与协作</strong>：<ul>
<li><strong>用户+服务器</strong>：用户本地做复杂的非线性操作，服务器做简单的线性操作（加减乘）。比如用户把加密后的 CT 数据发给服务器，服务器先做加减乘运算，把中间结果发回给用户，用户解密后做 ReLU 激活，再加密发回服务器，继续下一步运算。</li>
<li>两个<strong>不串通的服务器协作</strong>：将数据分割给两个互不串通的服务器，分别处理部分任务。同态加密负责线性运算，秘密共享负责处理复杂操作。两个服务器只能看到自己手里的部分数据，必须配合才能运算。</li>
</ul>
</li>
<li><strong>多项式近似</strong>：<ul>
<li>既然同态加密只认加乘，就把复杂操作改成能用加减乘表示的多项式，即用低阶多项式逼近非线性函数。</li>
<li>常用方法：泰勒展开、多项式逼近（Chebyshev 多项式）。</li>
<li>虽然近似后结果会有微小误差，但只要误差在可接受范围内，就能满足需求。</li>
</ul>
</li>
</ul>
<h3 id="3-2-1-多方安全计算"><a href="#3-2-1-多方安全计算" class="headerlink" title="3.2.1 多方安全计算"></a><strong>3.2.1 多方安全计算</strong></h3><p>两个百万富翁想知道他们两个谁更富有，但他们都不想让对方及其他第三方知道自己财富的任何信息，在这种情况下如何比较？这是姚启智教授<sup>[14]</sup> 于 1982 年提出的百万富翁问题，由此开创了密码学研究的新领域：安全多方计算（Secure Multi-party Computation，MPC）。</p>
<p>MPC 的核心目标是：让多个互不信任的参与者，在不泄露各自数据的前提下，共同完成计算任务，只输出最终结果。数据始终保存在本地，计算依靠加密协议协作完成，全程保证隐私安全。这完美契合了机器学习隐私保护的要求，于是近年来多方安全计算在机器学习中的应用是一个热门话题。</p>
<center><img src="https://dstbp.com/data/imgs/posts/PPML-Review/MPC.png" width="600" alt=""></center>


<center style="margin-top: 10px; margin-bottom: 18px;"><span style="font-family:楷体;font-size:14px;">图3.2 多方安全计算示意图</span></center>

<p>根据使用范围不同，MPC 的研究分为两类：</p>
<ol>
<li><strong>通用安全多方计算</strong><ul>
<li>特点：能处理任何计算任务（比如加减乘除、深度学习），只要能写成程序，就能在 MPC 框架下执行。</li>
<li>实现方式：通过混淆电路或秘密共享技术，把计算逻辑转化为加密电路，参与者协作计算。</li>
<li>优点：适用范围广，几乎能解决所有隐私计算问题。</li>
<li>缺点：效率低，计算复杂任务耗时很长（比如计算一个神经网络可能需要几小时）。</li>
</ul>
</li>
<li><strong>特定安全多方计算</strong><ul>
<li>特点：针对某个具体问题（比如计算交集、比较大小、求最大值）设计专门的算法，效率更高。</li>
<li>优点：效率高，适合高频次、低复杂度的任务。</li>
<li>缺点：适用范围窄，每个问题需要单独设计算法。</li>
</ul>
</li>
</ol>
<p>无论是通用 MPC 还是特定 MPC，都依赖三种核心技术实现隐私保护：</p>
<ol>
<li><p><strong>秘密共享（Secret Sharing）</strong></p>
<p>把一个数据拆成多个碎片，分发给不同参与者。只有收集到足够数量的碎片，才能还原出原始数据；单独一个碎片毫无意义。常用于分布式管理，比如银行保险柜的密码被拆成 3 份，分别交给 3 个管理员。只有至少 2 个管理员同时输入自己的碎片，才能打开保险柜。</p>
</li>
<li><p><strong>不经意传输（Oblivious Transfer）</strong></p>
<p>发送方给接收方多个选项（比如两个加密文件），接收方可以选择其中一个解密查看，但发送方不知道接收方选了哪个，接收方也看不到其他选项的内容。典型应用是隐私查询，比如医院 A 向医院 B 查询某患者是否在训练集中，但不暴露患者 ID。医院 B 就可以通过 OT 返回结果（“是”或“否”），同时医院 B 也无法得知查询的是哪个患者。</p>
</li>
<li><p><strong>混淆电路（Garbled Circuit）</strong></p>
<p>把计算逻辑（比如如果 A&gt;B，输出 1，否则输出 0）转化为一个加密的电路。参与者通过交换加密后的输入和电路信息，协作计算出结果，但没人能看到电路的具体结构或输入值。常用于需要保密比较的场景，比如你和朋友玩石头剪刀布，但都不想先暴露自己的选择。你们可以各自把选择加密，通过混淆电路协议比较，直接得到结果，但双方都不知道对方的原始选择。</p>
</li>
</ol>
<p>安全多方计算的优点很明显。优点是通过密码学协议确保数据不泄露，安全可靠；并且无需可信第三方，参与者直接协作；而且支持复杂计算，适用性广。但同样有缺点：</p>
<ol>
<li><strong>效率低</strong>：加密和解密过程耗时，处理大规模数据（如亿级样本）可能需要数天甚至更长时间。</li>
<li><strong>通信开销大</strong>：参与者需要频繁交换加密数据，网络带宽占用高。</li>
<li><strong>恶意模型支持不足</strong>：目前大多数方案只在半诚实模型（参与者按规则执行）下安全，而恶意模型的实用方案较少。</li>
</ol>
<h1 id="4-未来发展"><a href="#4-未来发展" class="headerlink" title="4. 未来发展"></a><strong>4. 未来发展</strong></h1><p>当前，机器学习的隐私保护正处于<strong>需求旺盛但技术受限</strong>的阶段：一方面，敏感数据保护的紧迫性不断提升；另一方面，现有技术在效率、兼容性和保护范围上仍存在不足。未来的研究将围绕“突破技术瓶颈、适配复杂场景、构建标准体系”三大核心方向展开，具体可细分为八个研究重点：  </p>
<ol>
<li><p><strong>训练阶段的加密技术</strong></p>
<p>现有加密方案（如同态加密）多应用于预测阶段，而训练阶段因计算复杂、效率低下，很少有应用。未来需探索在加密状态下高效训练模型的可能，比如改进同态加密的噪声管理、设计适配密文运算的轻量化神经网络，或结合拆分学习，将模型划分为客户端与服务器部分，仅对敏感中间结果加密。</p>
</li>
<li><p><strong>设计通用的隐私保护框架</strong></p>
<p>当前研究多针对特定攻击或应用场景，方法零散，缺乏统一体系。未来需构建覆盖“<strong>数据预处理—模型训练—推理应用”</strong>全流程的通用框架，实现<strong>协议—模型—系统</strong>的协同优化。重点方向包括：基于 GPU 与编译器的加速优化，以及加密协议的兼容性设计（如在同态加密与秘密共享之间实现无缝切换，适配多种场景）。</p>
</li>
<li><p><strong>保护非结构化数据</strong></p>
<p>目前的隐私保护研究多集中在表格型数据，但现实中大量数据为半结构化或非结构化（如图像、视频、社交关系、传感器信号），需要设计能保护这类复杂数据隐私的方法。比如对图像进行<strong>特征级加密</strong>（仅加密关键特征如五官比例），对文本设计<strong>隐私感知分词</strong>（屏蔽疾病、身份等敏感词），以在保护隐私的同时保留部分可用性。</p>
</li>
<li><p><strong>拓展到更多学习范式</strong></p>
<p>当前隐私研究主要集中在有监督学习，而<strong>无监督学习</strong>与<strong>强化学习</strong>应用日益广泛，其隐私问题却没被充分解决。未来研究需关注如：在聚类任务中引入<strong>类别混淆机制</strong>，避免通过群体特征反推出个体隐私；在强化学习中对<strong>试错日志脱敏</strong>，仅保留关键动作，减少策略或商业机密泄露。</p>
</li>
<li><p><strong>多技术融合</strong></p>
<p>这是现在的研究热门，单一技术往往存在局限，未来趋势是把这些技术结合起来。但融合不是简单的技术堆砌，而是<strong>优势互补</strong>，这里举几个例子：</p>
<ul>
<li>安全多方计算 + 区块链：利用区块链记录训练过程以实现可审计，同时由 MPC 保证隐私。</li>
<li>同态加密 + 差分隐私：前者保护训练过程，后者为最终模型参数加噪声。</li>
<li>TEE + 联邦学习：通过可信执行环境降低通信成本并增强参数聚合隐私性。</li>
</ul>
<p>如何设计更优雅的融合方案是我们需要考虑的。</p>
</li>
<li><p><strong>实现“单点 + 全局”的全维度保护</strong></p>
<p>传统研究多集中于单条数据的保护，但现实中隐私泄露往往来自数据关联。只隐藏某条社交关系没用，通过关系网拓扑仍能定位用户。未来要实现全局隐私保护，比如针对<strong>时许数据</strong>（如心电图、位置轨迹），设计时序混淆算法，让数据序列的整体规律不泄露；针对<strong>关联数据</strong>（如社交关系、供应链数据），通过节点匿名与结构扰动，既隐藏个体又不破坏数据的整体可用性。</p>
</li>
<li><p><strong>在隐私、效率、可用性之间找到平衡</strong></p>
<ul>
<li>差分隐私：效率高，但加噪声后结果可能“不好用”。</li>
<li>同态加密：隐私保护强，但计算太慢。</li>
<li><p>多方安全计算：能保证隐私和结果正确，但通信开销大。</p>
<p>每一个技术都有自己注重的地方，需要建立一个<strong>多维度的评估体系</strong>，明确什么时候该牺牲一点精度换隐私，什么时候该优先保证可用性。</p>
</li>
</ul>
</li>
<li><p><strong>统一的隐私泄露度量标准</strong></p>
<p>现在大家都在研究如何保护隐私，但很少有一个统一的方式去量化“隐私到底泄露了多少”。有的方法隐私威胁只算数据库中单个样本的泄露风险，而有的只测模型被逆向攻击的概率，缺乏统一标准。未来需要建立一套标准化的指标体系，既能反映攻击者的背景知识和数据关联性，又能跨算法、跨场景统一评估。这样才能在不同算法之间公平比较，也方便行业监管。比如可以基于<strong>信息熵</strong>设计量化方法。</p>
</li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>[隐私保护] 隐私保护机器学习综述</p><p><a href="http://dstbp.com/2025/09/23/PPML1/">http://dstbp.com/2025/09/23/PPML1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>DSTBP</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-09-23</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-09-23</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="" rel="noopener" target="_blank" title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/deed.zh">CC BY 4.0</a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/">数据安全</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/images/alipay_qrcode.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/images/wechat_qrcode.jpg" alt="微信"></span></a></div></div></div><div class="card"><nav class="post-navigation mt-4 level is-mobile card-content"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/07/30/msc/"><span class="level-item">[古典密码] 单表替换密码</span><i class="level-item fas fa-chevron-right"></i></a></div></nav></div><div class="card" id="comments"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content twikoo" id="twikoo"></div><script src="/js/imaegoo/twikoo/1.6.41/twikoo.min.js"></script><script>twikoo.init({
            envId: 'https://twikoo.dstbp.com',
            
            lang: "zh-CN",
            onCommentLoaded: function () {
              var commentContents = document.getElementsByClassName('tk-content');
              for (var i = 0; i < commentContents.length; i++) {
                var commentItem = commentContents[i];
                var imgEls = commentItem.getElementsByTagName('img');
                if (imgEls.length > 0) {
                  for (var j = 0; j < imgEls.length; j++) {
                    var imgEl = imgEls[j];
                    var aEl = document.createElement('a');
                    aEl.setAttribute('class', 'tk-lg-link');
                    aEl.setAttribute('href', imgEl.getAttribute('src'));
                    aEl.setAttribute('data-src', imgEl.getAttribute('src'));
                    aEl.appendChild(imgEl.cloneNode(false));
                    imgEl.parentNode.insertBefore(aEl, imgEl.nextSibling);
                    imgEl.remove();
                  }
                  if (typeof $.fn.lightGallery === 'function') {
                    $(commentItem).lightGallery({
                      selector: '.tk-lg-link'
                    });
                  }
                }
              }
            }
        });</script></div></div></div><style>.column.column-left,.column.column-right{display:none}</style><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar.png" alt="DSTBP"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">DSTBP</p><p class="is-size-6 is-block">r0xanne</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Elliðaey island. Iceland.</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">2</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/dstbp" target="_blank" rel="me noopener" id="widget-follow">GitHub</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Home" href="https://dstbp.net"><i class="fas fa-home"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Stackexchange" href="https://crypto.stackexchange.com/users/112358/dstbp"><i class="fab fa-stack-exchange"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Mail" href="mailto:TaddeoWang123@gmail.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget"><div class="card-content"><div class="media" style="overflow: hidden; align-items: center"><div class="media-left"><figure class="image is-96x96"><img src="/images/weixin/qrcode.jpg" alt="微信二维码"></figure></div><div class="media-content"><div id="weixin-search-logo"></div><div class="control has-icons-left" id="weixin-search-text"><input class="input" readonly value="三两黑白" onfocus="this.select()"><span class="icon is-small is-left"><i class="fas fa-search"></i></span></div></div></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">2025 年 9 月</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/07/"><span class="level-start"><span class="level-item">2025 年 7 月</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/05/"><span class="level-start"><span class="level-item">2025 年 5 月</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">2024 年 2 月</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">2024 年 1 月</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#引言"><span class="level-left"><span class="level-item">1</span><span class="level-item">引言</span></span></a></li><li><a class="level is-mobile" href="#1-基础知识"><span class="level-left"><span class="level-item">2</span><span class="level-item">1. 基础知识</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-机器学习"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">1.1 机器学习</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-1-概述"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">1.1.1 概述</span></span></a></li><li><a class="level is-mobile" href="#1-1-2-分类"><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">1.1.2 分类</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#2-机器学习中的隐私威胁"><span class="level-left"><span class="level-item">3</span><span class="level-item">2. 机器学习中的隐私威胁</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-隐私数据"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">2.1 隐私数据</span></span></a></li><li><a class="level is-mobile" href="#2-2-隐私威胁"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">2.2 隐私威胁</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-2-1-概述"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">2.2.1 概述</span></span></a></li><li><a class="level is-mobile" href="#2-2-2-数据窃取攻击"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">2.2.2 数据窃取攻击</span></span></a></li><li><a class="level is-mobile" href="#2-2-2-模型反演攻击"><span class="level-left"><span class="level-item">3.2.3</span><span class="level-item">2.2.2 模型反演攻击</span></span></a></li><li><a class="level is-mobile" href="#2-2-4-属性推理攻击"><span class="level-left"><span class="level-item">3.2.4</span><span class="level-item">2.2.4 属性推理攻击</span></span></a></li><li><a class="level is-mobile" href="#2-2-5-模型提取攻击"><span class="level-left"><span class="level-item">3.2.5</span><span class="level-item">2.2.5 模型提取攻击</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#3-机器学习中的隐私保护"><span class="level-left"><span class="level-item">4</span><span class="level-item">3. 机器学习中的隐私保护</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-隐私保护思想"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">3.1 隐私保护思想</span></span></a></li><li><a class="level is-mobile" href="#3-2-隐私保护技术"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">3.2 隐私保护技术</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-2-1-数据脱敏"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">3.2.1 数据脱敏</span></span></a></li><li><a class="level is-mobile" href="#3-2-2-差分隐私"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">3.2.2 差分隐私</span></span></a></li><li><a class="level is-mobile" href="#3-2-1-同态加密"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">3.2.1 同态加密</span></span></a></li><li><a class="level is-mobile" href="#3-2-1-多方安全计算"><span class="level-left"><span class="level-item">4.2.4</span><span class="level-item">3.2.1 多方安全计算</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#4-未来发展"><span class="level-left"><span class="level-item">5</span><span class="level-item">4. 未来发展</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2025/09/23/PPML1/"><img src="https://dstbp.com/data/imgs/posts/PPML-Review/Thumbnail.png" alt="[隐私保护] 隐私保护机器学习综述"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-09-23T05:55:20.000Z">2025-09-23</time></p><p class="title"><a href="/2025/09/23/PPML1/">[隐私保护] 隐私保护机器学习综述</a></p><p class="categories"><a href="/categories/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/">隐私保护</a> / <a href="/categories/%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/">数据安全</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/07/30/msc/"><img src="https://dstbp.com/data/imgs/posts/msc/Thumbnail.png" alt="[古典密码] 单表替换密码"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-07-30T11:58:43.000Z">2025-07-30</time></p><p class="title"><a href="/2025/07/30/msc/">[古典密码] 单表替换密码</a></p><p class="categories"><a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/">密码学</a> / <a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/">古典密码</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/07/14/steg-sub/"><img src="https://dstbp.com/data/imgs/posts/smsc/Thumbnail.png" alt="[古典密码] 隐写式单表替换密码"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-07-14T04:30:00.000Z">2025-07-14</time></p><p class="title"><a href="/2025/07/14/steg-sub/">[古典密码] 隐写式单表替换密码</a></p><p class="categories"><a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/">密码学</a> / <a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/">古典密码</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2025/05/14/lzw/"><img src="https://dstbp.com/data/imgs/posts/lzw/Thumbnail.png" alt="[信息编码] LZW 压缩算法"></a></figure><div class="media-content"><p class="date"><time dateTime="2025-05-14T07:10:43.000Z">2025-05-14</time></p><p class="title"><a href="/2025/05/14/lzw/">[信息编码] LZW 压缩算法</a></p><p class="categories"><a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/">密码学</a> / <a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/%E4%BF%A1%E6%81%AF%E7%BC%96%E7%A0%81/">信息编码</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/02/01/buuctf1/"><img src="https://dstbp.com/data/imgs/posts/buuctf/Thumbnail.png" alt="[日常训练] Crypto Training - BUUCTF（一）"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-02-01T00:10:43.000Z">2024-02-01</time></p><p class="title"><a href="/2024/02/01/buuctf1/">[日常训练] Crypto Training - BUUCTF（一）</a></p><p class="categories"><a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/">密码学</a> / <a href="/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/%E6%97%A5%E5%B8%B8%E8%AE%AD%E7%BB%83/">日常训练</a></p></div></article></div></div><div class="card widget" id="twikoo-new"><div class="card-content"><div class="menu"><h3 class="menu-label">最新评论</h3><script>window.twikooEnvId = 'https://twikoo.dstbp.com';</script><div class="twikoo-new-container"></div></div></div></div></div><style>.column.column-left,.column.column-right{display:block}</style></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img class="logo-img" src="/images/logo.jpg" alt="DSTBP Blog" height="28"><img class="logo-img-dark" src="/images/logo-dark.jpg" alt="DSTBP Blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 DSTBP</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/imaegoo/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">总访问量&nbsp;<span id="busuanzi_value_site_pv">-</span>&nbsp;次&nbsp;&nbsp;总访客数&nbsp;<span id="busuanzi_value_site_uv">-</span>&nbsp;人</span></p><p class="is-size-7">版权说明：本网站所有内容版权归属 DSTBP，仅限网友与自己学习交流，如有侵权，请<a href="/message" target="_blank" rel="noreferrer noopener">留言</a>，管理员立即处理<br/><a href="https://beian.miit.gov.cn" target="_blank" rel="noreferrer noopener">萌 ICP 备 20211214 号-8</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/deed.zh"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="/js/imaegoo/instant.page/5.1.1/instantpage.min.js" type="module"></script><script src="/js/imaegoo/jquery/3.3.1/dist/jquery.min.js"></script><script src="/js/imaegoo/moment/2.22.2/min/moment-with-locales.min.js"></script><script src="/js/imaegoo/twikoo/1.6.41/twikoo.min.js"></script><script src="/js/imaegoo/clipboard/2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="/js/imaegoo/lightgallery/1.10.0/dist/js/lightgallery.min.js" defer></script><script src="/js/imaegoo/justifiedGallery/3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="/js/imaegoo/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script src="/js/imaegoo/cookieconsent/3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-right",
        content: {
          message: "此网站使用 Cookie，以启用评论系统和分析功能。",
          dismiss: "知道了",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "/cookies/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><div class="searchbox-pinyin"><label class="checkbox"><input id="search-by-pinyin" type="checkbox" checked="checked"><span> 拼音检索</span></label></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/imaegoo/pinyin.js" defer></script><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script type="text/javascript" src="/js/imaegoo/imaegoo.js"></script><script type="text/javascript" src="/js/imaegoo/universe.js"></script></body></html>